# Chapter 6. Monitoring Distributed Systems

##  Worrying About Your Tail (or, Instrumentation and Performance)

あなたのしっぽについて悩む

あるいは、<br>
Instrumentation = ハードウェアやソフトウェアの状態に関するデータを報告するために使用される機構<br>
Performance = パフォーマンス

この章では、しっぽをどう監視するか？
「しっぽ」とは、パフォーマンスのもっとも悪い値のことを指す

スクラッチで監視システムを構築する場合、いくつかの平均に基づいて設計するのが良いです

* latency(待ち時間)の平均
* ノードのCPU使用率の平均
* データベース負荷(fullness)の平均

だったり。
毎秒1,000件のリクエストを平均待ち時間：100ミリ秒で応答するWebサービスを実行する場合、そのうち1%は5秒かかるかもしれません。

この非常に遅い、しっぽとなるリクエストを区別する方法は
待ち時間によって分類されたリクエスト数をカウントします

* 0ms - 10ms
* 10ms - 30ms
* 30ms - 100ms
* 100ms - 300ms

のリクエスト数がどのくらいか？
そうすることでリクエストの分布を可視化することができます。

## Choosing an Appropriate Resolution for Measurements

計測のための適切な解決を選択する

この章では、システムを監視する中で色々な側面、aspectがあるが
それをどう適切に計測、解決するか？

システムの異なった側面、aspectは、それぞれ粒度の異なるレベルで測定されるべきです

例えば
* 1分毎のCPUロードを監視した場合、瞬間的に高い（スパイクな）値を明確できない
* 一方、99.9%稼働、年間9時間未満のダウンタイムとするWebサービスで、1,2分間ごとに ステータス200 を応答するか監視することはあるが、1,2分間ごとにそのWebサービスのHDを監視することは不要

どのように測定の粒度を構成するかを注意してください

CPU負荷の毎秒の測定値を収集することは、興味深いデータが得られるかもしれないが、頻繁にcollect, store, analyzeすることは非常にコストが高額になるかもしれない

もし、高度な解決を求めるが少しの遅延があっても良いのであれば、
取得したデータをいくつか内部的にサンプリングすれば、コストを削減することができる。

1. 毎秒のCPU使用率を記録
2. 5％の粒度のbucketsを使用して、毎秒ごとの適切なCPU使用のbucketをインクリメントする。
3. 毎分それらの値を集計する

高コスト化を招くことなく、簡単なCPUのhotspotsを観察することができる

## As Simple as Possible, No Simpler

可能な限りシンプルに

この章では、監視システムのデータ、収集・集約・アラートの構成は複雑化せず、よりシンプルに構成すべきである。と記述

監視システムであらゆる要件を積み上げると、非常に複雑な監視システムになるかもしれない。

* latencyがしきい値と異なる、パーセンタイルが異なる、すべての種類の異なるメトリックスが発生した場合にアラートを発する

* 障害を検出し、考えられる要因を露出するためのExtra codeがある
* これらの考えられる要因のそれぞれについての関連するダッシュボード

潜在的な複雑さは終わることがない
すべてのソフトウェアシステムと同様、監視はとても複雑になり、それは壊れやすく、変更が複雑で、保守の負担があり、単純化に向けて監視システムを設計する必要がある

監視するために何を選択するかは、次のガイドラインに従ってください

* 実際の出来事をキャッチするルールは、可能な限り、シンプルで予測可能、かつ信頼できるものであるべき

* めったに実施されていないデータの収集、集約、およびアラート構成はできれば除去すべきです

* 収集したが、prebake？されたダッシュボートに露出していない、任意の警告で使用されていないシグナルは、除去の候補です

Googleの経験では、アラートおよびダッシュボードとペアでmetricsの基本的な収集と集計は、スタンドアロンなシステムとして動作している

監視と複雑なシステムの色々な側面を組み合わせると、魅力的なことができるが
複雑で壊れやすく、多くの結果が混在する

ソフトウェアエンジニアリングと同様、明確で、単純で、疎結合なシステムを維持することはより良い戦略

## Tying These Principles Together

ともにこれらの原則を型付けすること

この章では監視とアラートの原則について記述
この原則は広くGoogleのSREチーム内で承認し、続いている

監視とアラートのルールを作成するとき、次の質問をすると false positivesとpager burnoutを避けるのを助けることができる

false positives・・・誤った警告メッセージ
pager burnout・・・呼び出しによって疲労する

* このルールは、緊急、actionable（訴訟になる）、もしくはユーザの目に見えていて差し迫っている　以外は検出されない状態か？

* この警告は無視することができるか？いつ、そしてなぜこの警告を無視することができ、どのようにこのシナリオを回避することができるか？

* この警告は間違いなく、ユーザーがマイナスの影響を受けていることを示している？

* この警告に応答して行動を取ることはできますか？そのアクションが急務か、それとも朝まで待つことができるか？アクションは安全に自動化することができるか？そのアクションは、長期的な修正、または単に短期的な回避策になるか？

* 他の人々はこのissueのために呼び出されるか？ それによって、呼び出しの少なくとも一つは不益か？

次に、page呼び出しの基本的な哲学を記述

* 呼び出しが鳴り出すたびに、私は切迫感を感じる必要がある
* すべての呼び出しは実用的であるべき
* 各呼び出しの応答は知性を要求すべきで、ロボット的な応答でよければ、それは呼び出しであるべきではない
* 呼び出しは、新規の問題か前に見られていないイベントであるべき

この哲学を満たしていないのであれば、
一定の判定と分散させ、また判断を増幅させるので、監視によってトリガされべきではない

## Monitoring for the Long Term

長期的な監視

この章では、監視に関する２つのケーススタディを使用して
監視に関する決定を長期的な目標とすることが重要であることを記述

### Bigtable SRE: A Tale of Over-Alerting

Bigtable SRE：多すぎるアラートの逸話

Googleの内部インフラストラクチャはサービスレベルの目標（=SLO）が設定されていて、Bigtableのサービスもクライアントの平均的なパフォーマンスに基づいて設定されていた

Bigtableやストレージ・スタックの問題で、リクエストのワースト5％は著しく遅いケースがあり、SLOが近づくと電子メールのアラートがトリガされ、SLOを超えたときにポケベルへの通知がトリガされた

この両方のアラートが大量に発射され、エンジニアが許容できない量に達し
アラートの分類にかなりの時間を費やしました。そして、実際にユーザーに影響を与えた問題を逃しました

状況を改善するために、
* Bigtableのパフォーマンスを向上させるために大きな努力をしながら、
一時的にSLO目標を下方修正（dialed back）しました
* 多くの時間を費やし、アラートを診断することが不可能だったので Eメールでのアラートを無効化しました

この戦略により、Bigtableの長期的な問題を修正するだけでなく、常に戦術的な問題を解決するのに十分な余裕を与えました。

すべての時間帯で通知に追いついていなかったが、
この改善によりオンコールエンジニアが実際に作業を行うことができました。

### Gmail: Predictable, Scriptable Responses from Humans

Gmailの非常に初期の頃、Workqueueと呼ばれるプロセス管理システム上に構築されていました

Workqueueは比較的不透明なコードベースでバグがありました

Gmailの監視は、タスクが「de-scheduled」になったとき、アラートを発火するよう、構築されました

他にたくさんのタスクを持っていてアラートの保守ができないので、
さらにschedulerを「poke」（突っつく） するツールを構築しました。

チームの中で、問題の検出からreschedulerを実行するまでの全ループを
シンプルに自動化すべきかどうか議論しました
これはあくまで回避策であり、本来のバグの修正を遅らせる心配があり
保守不可能な技術的負債になり得るという懸念がありました。

マネージャーとテクニカルリーダーがキーとなり、優先して、本当の長期的な修正を実行します
また、繰り返し、アルゴリズムがある通知は、警告を促す「red flag」であるべきで
そのような通知を自動化することは、技術的負債をクリーンアップすることの自信に欠けている。

## The Long Run

この章では、前の章と同様、長期的な戦略が重要であることを記述

膨大な努力はガタガタのシステムの高可用性を成し遂げることができるが
この方法は短期的で燃え尽きる危険をはらんでいて、少数の英雄的メンバーに依存します

短期的なコントロールを減らすと痛みを伴うが、これはシステムの長期安定性のために戦略的なトレードで、単独のイベントとしてすべての通知を考えないことが重要です。

あと、
四半期ごとのレポートで、意思決定者が通知の負荷と彼らのチームの全体的な健全性に関して最新状態が保たれていること

## Conclusion

結論

Eメールの警告は、非常に制限された価値をもち、容易に、ノイズがはびこる傾向がある

代わりに、ダッシュボードを支持するべき、ダッシュボードで、進行中で露出前の問題を監視します
ダッシュボードはまた歴史の相互関係を分析するために、ログとともにペアで構成しても良い

長い目で見ると、on-call ローテーションの成功の実現と 徴候または差し迫っている本当の問題のアラートの選択を含む製品は 実際達成可能なゴールのために、あなたの目標を適応させ、 あなたの監視の急速診断をサポートすることを確実にします。

<hr>

# Chapter 7. The Evolution of Automation at Google

Googleの自動化の進化

SRE にとって、自動化は力を増強させる食べ物であるが万能薬ではない
不注意に自動化をすることは、それを解くのと同じくらい多くの問題が生じる

自動化の価値は、それが何をするかとその賢明なアプリケーションの両方から来ていて。
自動化の価値と　どのような姿勢で進化してきたのか　の両方を説明します。

## The Value of Automation　　

自動化の価値

自動化の価値は何ですか？

### Consistency

一貫性

手動でのタスクだと、毎回同じ方法で実行されないが、自動化することで一貫性を確保できます
一貫性の欠如は、データの品質や信頼性の問題で、ミス、見落とし、問題につながります

### A Platform

プラットフォーム

正しく設計された自動化システムは、拡張可能なプラットフォームを提供し
多くのシステムに適用され、または利益のために長く保たれます

また、プラットフォームは、誤りを一元化します。
バグがあれば、一度の修正で永遠に修正されます。
また、追加のタスクを実行するように拡張することもできます。

手動でのタスクと異なり、頻繁に連続的に、
また時として人間には不便な時間にも実行できます

その上、性能についてのメトリックスをエクスポートすることができ
以前は知らなかった手順についての詳細を発見することができる

### Faster Repairs

より早く修理する



### Faster Action
### Time Saving
### WARNING

## The Value for Google SRE

## The Use Cases for Automation

## Google SRE’s Use Cases for Automation

## A Hierarchy of Automation Classes

## Automate Yourself Out of a Job: Automate ALL the Things!
